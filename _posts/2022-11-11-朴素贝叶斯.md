---
layout: post
title:  "朴素贝叶斯"
date:   2022-11-11 08:00:00 +0800
categories: 机器学习
keywords: 机器学习
description: 机器学习
commentId: 2022-11-11
---
> **朴素贝叶斯**
>
> 优点：在数据较少的情况下仍然有效，可以处理多类别问题。
>
> 缺点：对于输入数据的准备方式较为敏感。
>
> 适用数据类型：标称型数据。

朴素贝叶斯是贝叶斯决策理论的一部分，所以讲述朴素贝叶斯之前有必要快速了解一下贝叶斯决策理论。假设现在我们有一个数据集，它由两类数据组成，数据分布如图所示：

<center><img src="{{site.baseurl}}/pic/machine_learning/bayes/1.svg" width="50%"/></center>

我们现在用$p1(x,y)$表示数据点$(x,y)$属于类别1（图中用圆点表示的类别）的概率，用$p2(x,y)$表示数据点$(x,y)$属于类别2（图中用三角形表示的类别）的概率，那么对于一个新数据点$(x, y)$，可以用下面的规则来判断它的类别：

* 如果$ p1(x,y) > p2(x,y) $，那么类别为1
* 如果$ p2(x,y) > p1(x,y) $，那么类别为2

也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。

## 条件概念

概率论条件概率公式如下，

$$ p(AB) = p(A|B) \cdot p(B) $$

**贝叶斯准则**告诉我们如何交换条件概率中的条件与结果，如果已知$p(A\|B)$，要求$p(B\|A)$，可以用下面的方法：

$$ p(B|A) = \cfrac{p(AB)}{p(A)} = \cfrac{p(A|B) \cdot p(B)}{p(A)}$$

给定某个由$(x,y)$表示的数据点，那么该数据点来自类别c1的概率是多少？数据点来自类别c2的概率又是多少？使用贝叶斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到：

$$ p(c_{i}|x,y) = \cfrac{p(x,y|c_{i}) \cdot p(c_{i})}{p(x,y)} $$

* 如果$p(c_{1}\|x,y) > p(c_{2}\|x,y)$，那么属于类别c1

* 如果 $p(c_{1}\|x,y) < p(c_{2}\|x,y)$，那么属于类别c2

使用贝叶斯准则，可以通过已知的三个概率值来计算未知的概率值。

## 对文档进行分类

机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。要从文本中获取特征，需要先拆分文本。具体如何做呢？这里的特征是来自文本的词条（token），一个词条是字符的任意组合。

函数loadDataSet()创建了一些实验样本。该函数返回的第一个变量是进行词条切分后的文档集合，函数返回的第二个变量是一个类别标签的集合。这里有两类，侮辱性和非侮辱性。这些文本的类别由人工标注，这些标注信息用于训练程序以便自动检测侮辱性留言。

```python
def loadDataSet():
    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not
    return postingList,classVec

def createVocabList(dataSet):
    vocabSet = set([])  #create empty set
    for document in dataSet:
        vocabSet = vocabSet | set(document) #union of the two sets
    return list(vocabSet)

def setOfWords2Vec(vocabList, inputSet):
    returnVec = [0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] = 1
        else: print "the word: %s is not in my Vocabulary!" % word
    return returnVec
```

函数createVocabList()会创建一个包含在所有文档中出现的不重复词的列表。获得词汇表后，便可以使用函数setOfWords2Vec()，该函数的输入参数为词汇表及某个文档，输出的是文档向量，向量的每一元素为1或0，分别表示词汇表中的单词在输入文档中是否出现。

#### 从词向量计算概率

前面介绍了如何将一组单词转换为一组数字，接下来看看如何使用这些数字计算概率。现在已经知道一个词是否出现在一篇文档中，也知道该文档所属的类别。还记得前面提到的贝叶斯准则？我们重写贝叶斯准则，将之前的x、y替换为w。w表示这是一个向量，即它由多个数值组成。在这个例子中，数值个数与词汇表中的词个数相同。

$$ p(c_{i}|w) = \cfrac{p(w|c_{i}) \cdot p(c_{i})}{p(w)} $$

我们将使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。如何计算呢？首先可以通过类别i（侮辱性留言或非侮辱性留言）中文档数除以总的文档数来计算概率$p(c_{i})$。

接下来计算$p(w\|c_{i})$，这里就要用到朴素贝叶斯假设。如果将w展开为一个个独立特征，那么就可以将上述概率写作$p(w_{0}, w_{1}, w_{2}..w_{n}\|c_{i})$。这里假设所有词都互相独立，该假设也称作条件独立性假设，它意味着可以使用$p(w_{0}\|c_{i})p(w_{1}\|c_{i})p(w_{2}\|c_{i})...p(w_{n}\|c_{i})$来计算上述概率，这就极大地简化了计算的过程。

```python
def trainNB0(trainMatrix,trainCategory):
    numTrainDocs = len(trainMatrix)
    numWords = len(trainMatrix[0])
    pAbusive = sum(trainCategory)/float(numTrainDocs)
    p0Num = zeros(numWords); p1Num = zeros(numWords)
    p0Denom = 0.0; p1Denom = 0.0
    for i in range(numTrainDocs):
        if trainCategory[i] == 1:
            p1Num += trainMatrix[i]
            p1Denom += sum(trainMatrix[i])
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
    p1Vect = p1Num/p1Denom
    p0Vect = p0Num/p0Denom
    return p0Vect,p1Vect,pAbusive
```

代码函数中的输入参数为文档矩阵trainMatrix，以及由每篇文档类别标签所构成的向量trainCategory。首先，计算文档属于侮辱性文档（class=1）的概率，即$P(1)$。因为这是一个二类分类问题，所以可以通过$1-P(1)$得到$P(0)$。对于多于两类的分类问题，则需要对代码稍加修改。

计算$p(w_{i}\|c_{1})$和$p(w_{i}\|c_{0})$，需要初始化程序中的分子变量和分母变量。由于w中元素如此众多，因此可以使用NumPy数组快速计算这些值。上述程序中的分母变量是一个元素个数等于词汇表大小的NumPy数组。在for循环中，要遍历训练集trainMatrix中的所有文档。一旦某个词语（侮辱性或正常词语）在某一文档中出现，则该词对应的个数（p1Num或者p0Num）就加1，而且在所有的文档中，该文档的总词数也相应加1。对于两个类别都要进行同样的计算处理。最后，对每个元素除以该类别中的总词数。利用NumPy可以很好实现，用一个数组除以浮点数即可，若使用常规的Python列表则难以完成这种任务，读者可以自己尝试一下。最后，函数会返回两个向量和一个概率。

#### 根据现实情况修改分类器

利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算$p(w_{0}\|c_{1})p(w_{1}\|c_{1})p(w_{2}\|c_{1})$。如果其中一个概率值为0，那么最后的乘积也为0。为降低这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。

```python
def trainNB0(trainMatrix,trainCategory):
    numTrainDocs = len(trainMatrix)
    numWords = len(trainMatrix[0])
    pAbusive = sum(trainCategory)/float(numTrainDocs)
    p0Num = ones(numWords); p1Num = ones(numWords)
    p0Denom = 2.0; p1Denom = 2.0
    for i in range(numTrainDocs):
        if trainCategory[i] == 1:
            p1Num += trainMatrix[i]
            p1Denom += sum(trainMatrix[i])
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
    p1Vect = p1Num/p1Denom
    p0Vect = p0Num/p0Denom
    return p0Vect,p1Vect,pAbusive
```

另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积$p(w_{0}\|c_{i})p(w_{1}\|c_{i})p(w_{2}\|c_{i})...p(w_{n}\|c_{i})$时，由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。（读者可以用Python尝试相乘许多很小的数，最后四舍五入后会得到0。）一种解决办法是对乘积取自然对数。在代数中有$ln(a＊b)=ln(a)+ln(b)$，于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。

```python
def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):
    p1 = sum(vec2Classify * log(p1Vec)) + log(pClass1)
    p0 = sum(vec2Classify * log(p0Vec)) + log(1.0 - pClass1)
    if p1 > p0:
        return 1
    else:
        return 0

def testingNB():
    listOPosts,listClasses = loadDataSet()
    myVocabList = createVocabList(listOPosts)
    trainMat=[]
    for postinDoc in listOPosts:
        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))
    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))
    testEntry = ['love', 'my', 'dalmation']
    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))
    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)
    testEntry = ['stupid', 'garbage']
    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))
    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)
```

classifyNB函数的第一个参数是待判断的文档向量。假设该文档中出现了三个词，在词汇表中的位置分别是0，2，5，那就相当于是求：

$$ p(c_{i}|w_{0},w_{2},w_{5}) = \cfrac{p(w_{0},w_{2},w_{5}|c_{i}) \cdot p(c_{i})}{p(w_{0},w_{2},w_{5})} = \cfrac{p(w_{0}|c_{i}) \cdot p(w_{2}|c_{i}) \cdot p(w_{5}|c_{i}) \cdot p(c_{i})}{p(w_{0},w_{2},w_{5})}$$

#### 文档词袋模型

在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。为适应词袋模型，需要对函数setOfWords2Vec()稍加修改：

```python
def setOfWords2Vec(vocabList, inputSet):
    returnVec = [0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] += 1
        else: print "the word: %s is not in my Vocabulary!" % word
    return returnVec
```
